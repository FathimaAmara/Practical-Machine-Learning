---
title: "PracticalML"
author: "AmaraZuffer"
date: "2/10/2020"
output: html_document
---

Loading necessary libraries
```{r}
#imports
library(caret)
library(lattice)
library(ggplot2)
library(rpart)
library(gbm)
```

Loading the respective datasets

The datasets used in this exercise were downloaded and accessed via the local computer. They are differentiated as traina and test sets.
##Reading data
```{r}
data_train <- read.csv("data/pml-training.csv")
data_test <- read.csv("data/pml-testing.csv")
dim(data_train)
dim(data_test)
```


Data Preparation

In order to use this dataset for predictions, it is quite necessary to clean and format the dataset as per the requirement.

First the variables having near zero values are removed.
```{r}
#removing variables with near zero values
#is.na(data_train)
nzv = nearZeroVar(data_train)
org_training_data = data_train[,-nzv]
org_testing_data = data_test[,-nzv]

dim(org_training_data)
dim(org_testing_data)
```


Next, tuples with NA values are removed.(threshold used - 95%)
```{r}
#removing variables wiht NA values
na_vals <- sapply(org_training_data, function(x) mean(is.na(x))) >0.95
org_training_data <- org_training_data[,na_vals == FALSE]
org_testing_data <- org_testing_data[,na_vals == FALSE]
```
```{r}
dim(org_testing_data)
```


```{r}
dim(org_training_data)
```

Finally variables that are non-numeric are removed from the dataset.
```{r}
#remove unrelated variables
org_training_data <- org_training_data[,8:59]
org_testing_data <- org_testing_data[,8:59]
```
```{r}
dim(org_training_data)
```


```{r}
dim(org_testing_data)
```
```{r}
colnames(org_training_data)
```


```{r}
colnames(org_testing_data)
```


Seggregating Data


Here, the loaded training dataset is being divided in to training and testing.
```{r}
from_train <- createDataPartition(org_training_data$classe,p=0.6,list=FALSE)
train <- org_training_data[from_train,]
test <- org_training_data[-from_train,]

dim(train)
```
```{r}
dim(test)
```


Fitting different models to the datasets


1) Decision Tree Model

```{r}
library(randomForest)
D_Tree <- train(classe ~.,data=train, method='rpart')
```
```{r}
#prediction
D_Tree_pred <- predict(D_Tree,test)
confusionMatrix(D_Tree_pred,test$classe)
```
As per the overall metrics, the accuracy is very low. 


2)Random Forest Model

```{r}
RF_Model <- train(classe ~., data=train,method='rf',ntree=100)
```
```{r}
#prediction
RF_pred <- predict(RF_Model,test)
RF_pred_conf <- confusionMatrix(RF_pred,test$classe)
RF_pred_conf
```


```{r}
#plot
plot(RF_pred_conf$table, col = RF_pred_conf$byClass,main=paste("Random Forest - Accuracy=",round(RF_pred_conf$overall['Accuracy'],4)))
```

As per the accuracy metrics this model shows an accuracy of 99% on the test set which is very good. But, there is a concern on overfitting!


3)Gradient Boost Model

```{r warning=TRUE}
GB_model <- train(classe ~.,data=train ,method = 'gbm',verbose=FALSE)
GB_model$finalModel
```
```{r}
#prediction
GBM_pred <- predict(GB_model,test)

GBM_pred_accuracy <- confusionMatrix(GBM_pred, test$classe)
GBM_pred_accuracy
```

```{r}
#plot
plot(GBM_pred_accuracy$table, col=GBM_pred_accuracy$byClass, main=paste("Gradient Boost Model - Accuracy = ",round(GBM_pred_accuracy$overall['Accuracy'],4)))
```
The confusion metrics shows an overall accuracy of 96% which is acceptable.


Conclusion

As per our training and testing rounds it is clear that the Random Forest Model outperforms the Gradient Boost Model with an overall accuracy of 0.99 where as the GBM diplayed an accuracy of 0.96.




Applying Test Data on the selected (Random Forest) model

```{r}
final_pred <- predict(RF_Model, org_testing_data)
final_pred
```


